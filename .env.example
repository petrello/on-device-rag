# ============================================================================
# PATHS
# ============================================================================
DATA_DIR=data
MODELS_DIR=models
EMBEDDINGS_DIR=embeddings
STORAGE_DIR=storage

# ============================================================================
# VECTOR STORE CONFIGURATION
# ============================================================================
# Options: qdrant | faiss
# - qdrant: Production-grade, requires Docker, better accuracy
# - faiss: Faster, offline-capable, 30% less memory
VECTOR_STORE_TYPE=qdrant

# Qdrant settings (when VECTOR_STORE_TYPE=qdrant)
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=edge_rag

# FAISS settings (when VECTOR_STORE_TYPE=faiss)
FAISS_INDEX_PATH=storage/faiss_index

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Embedding Model
# Recommended options:
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims, 80MB, fast)
# - sentence-transformers/all-MiniLM-L12-v2 (384 dims, 120MB, more accurate)
# - ibm-granite/granite-embedding-278m-multilingual (768 dims, multilingual)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384

# LLM Model Path (relative to MODELS_DIR)
# Download from: https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF
# Quantization recommendations:
# - Q4_K_M: Best balance (1.2GB)
# - Q3_K_M: Lower memory (900MB), slightly less accurate
# - Q5_K_M: Higher accuracy (1.4GB), more memory
LLM_MODEL_PATH=models/Llama-3.2-1B-Instruct-Q4_K_M.gguf

# Hugging Face cache directory
HF_HOME=embeddings

# ============================================================================
# LLM PARAMETERS (Performance Tuning)
# ============================================================================

# Temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.0

# Maximum tokens to generate per response
# Lower values = less memory, faster responses
# Typical: 1024-2048 for concise answers, 2048-4096 for detailed
LLM_MAX_TOKENS=2048

# Context window size
# Lower values = less memory
# Must be >= max input + LLM_MAX_TOKENS
LLM_CONTEXT_WINDOW=4096

# CPU threads to use for inference
# Auto-detect if not set, or set to number of physical cores
# Typical: 4 for quad-core, 8 for octa-core
LLM_THREADS=4

# Batch size for processing
# Lower values = less memory, slightly slower
# Typical: 256-512
LLM_BATCH_SIZE=256

# ============================================================================
# CHUNKING STRATEGY
# ============================================================================

# Enable hierarchical chunking (recommended)
# Retrieves small precise chunks but provides large context to LLM
USE_HIERARCHICAL_CHUNKING=true

# Child chunk size (for retrieval)
# Smaller = more precise but may lack context
# Larger = more context but less precise
# Typical: 300-500
CHUNK_SIZE=400

# Overlap between chunks
# Prevents information loss at boundaries
# Typical: 10% of CHUNK_SIZE
CHUNK_OVERLAP=40

# Parent chunk size (for LLM context)
# Only used when USE_HIERARCHICAL_CHUNKING=true
# Should be 2-3x CHUNK_SIZE
PARENT_CHUNK_SIZE=1200

# ============================================================================
# RETRIEVAL CONFIGURATION
# ============================================================================

# Number of chunks to retrieve per query
# More = better recall but slower, more tokens used
# Typical: 3-5
SIMILARITY_TOP_K=3

# Enable hybrid search (vector + BM25)
# Combines semantic and keyword-based retrieval
# Recommended: true for better accuracy
USE_HYBRID_SEARCH=true

# Hybrid search alpha parameter
# 1.0 = pure vector search (semantic only)
# 0.5 = equal weight
# 0.0 = pure BM25 (keyword only)
# Recommended: 0.6-0.8
HYBRID_ALPHA=0.7

# Enable reranking (requires additional model)
# Reranks retrieved results for better relevance
# Set to false to save memory
ENABLE_RERANKING=false

# ============================================================================
# MEMORY MANAGEMENT
# ============================================================================

# Maximum chat history to keep in memory
# Each exchange (user + assistant) counts as 2 messages
# Lower = less memory
MAX_CHAT_HISTORY=10

# Memory threshold for automatic cleanup (MB)
# Triggers garbage collection when exceeded
# Set to 75% of available RAM
AUTO_CLEANUP_THRESHOLD_MB=6000

# Enable memory profiling (requires psutil)
# Shows RAM usage in UI
# Useful for debugging, disable in production
ENABLE_MEMORY_PROFILING=false

# ============================================================================
# CONVERSATIONAL MEMORY
# ============================================================================

# Enable multi-turn conversation context
# Maintains context across queries (handles "it", "that", follow-ups)
ENABLE_CONVERSATION_MEMORY=true

# Number of previous exchanges to include in context
# 1 exchange = 1 user message + 1 assistant response
# Higher = better context but more tokens used
# Typical: 3-5
MEMORY_WINDOW=5

# ============================================================================
# CITATION EXTRACTION
# ============================================================================

# Enable automatic citation extraction
# Links answers to source documents
ENABLE_CITATIONS=true

# Characters to show before/after cited text
# Higher = more context, more space used
# Typical: 150-300
CITATION_HIGHLIGHT_LENGTH=200

# ============================================================================
# MONITORING & METRICS
# ============================================================================

# Enable Prometheus metrics endpoint
# Exposes metrics at http://localhost:8001/metrics
ENABLE_METRICS=true

# Metrics server port
METRICS_PORT=8001

# Logging format
# Options: json | text
# json = structured logging for production
# text = human-readable for development
LOG_FORMAT=json

# Logging level
# Options: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# ============================================================================
# DOCUMENT MANAGEMENT
# ============================================================================

# Maximum file upload size (MB)
MAX_UPLOAD_SIZE_MB=50

# Allowed file extensions (comma-separated)
ALLOWED_EXTENSIONS=pdf,txt,docx,md

# Enable automatic document refresh
# Re-indexes documents when files change
AUTO_REFRESH_DOCUMENTS=false

# ============================================================================
# SECURITY
# ============================================================================

# Enable authentication (requires streamlit-authenticator)
ENABLE_AUTH=false

# API rate limiting (queries per minute per user)
RATE_LIMIT_PER_MINUTE=30

# Enable CORS (for API access)
ENABLE_CORS=false

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000

# ============================================================================
# ADVANCED SETTINGS (Usually don't need to change)
# ============================================================================

# Sentence splitting strategy
# Options: sentence | paragraph | semantic
SPLIT_STRATEGY=sentence

# Vector similarity metric
# Options: cosine | euclidean | dot_product
SIMILARITY_METRIC=cosine

# Minimum confidence score for retrieval results
# Results below this threshold are filtered out
# Range: 0.0-1.0
MIN_CONFIDENCE_SCORE=0.0

# Enable query caching
# Caches recent queries to speed up repeated questions
ENABLE_QUERY_CACHE=true

# Query cache size (number of queries)
QUERY_CACHE_SIZE=100

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================

# Enable query rewriting
# Automatically rewrites queries for better retrieval
ENABLE_QUERY_REWRITING=false

# Enable multi-query generation
# Generates multiple variations of query for better recall
ENABLE_MULTI_QUERY=false

# Number of query variations to generate
NUM_QUERY_VARIATIONS=3